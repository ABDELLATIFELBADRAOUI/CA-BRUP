# -*- coding: utf-8 -*-
"""CA-BRUP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18w92xG_GEaIosIJx8qzKU3UNL83mnczz
"""

# cabrup.py

import numpy as np
from scipy.special import gammaln
from scipy.stats import norm

class CA_RUP:
    """
    Covariate‐Augmented Bivariate Reinforced Urn Process (CA‐RUP)
    with Robust Bayesian inference via Metropolis–Hastings MCMC.
    """

    def __init__(self,
                 alpha0: float = 2.0,
                 D0: float = 2.0,
                 gamma_vec: np.ndarray = None,
                 delta_vec: np.ndarray = None,
                 covariates: np.ndarray = None,
                 r: float = 1.0,
                 censor_rate: float = 0.35,
                 max_steps: int = 20,
                 seed: int = None):
        self.alpha0 = alpha0
        self.D0 = D0
        self.gamma = np.array(gamma_vec) if gamma_vec is not None else np.zeros(0)
        self.delta = np.array(delta_vec) if delta_vec is not None else np.zeros(0)
        self.X = np.array(covariates) if covariates is not None else np.zeros((0, 0))
        self.r = r
        self.censor_rate = censor_rate
        self.max_steps = max_steps
        self.rng = np.random.default_rng(seed)

        self.n, self.p = self.X.shape if self.X.size else (0, 0)
        self.param_history = {'alpha0': [], 'D0': [], 'gamma': [], 'delta': []}
        self.log_likelihood_hist = []
        self.convergence_diag = {'R_hat': None, 'ESS': {}}

        self._init_urns()
        self._calibrate_censor_times()

    def _init_urns(self):
        """Initialize urn counts with covariate effects."""
        self.B = np.full(self.n, self.alpha0, dtype=float)
        self.R = np.full(self.n, self.D0, dtype=float)
        if self.n and self.p:
            surv_lin = np.exp(self.X @ self.gamma)
            death_lin = np.exp(self.X @ self.delta)
            self.B = self.alpha0 * surv_lin
            self.R = self.D0 * death_lin

    def _calibrate_censor_times(self):
        """Front-load censoring to target censor_rate."""
        cutoff = max(1, int(self.max_steps * self.censor_rate))
        self.censor_times = self.rng.integers(1, cutoff + 1, size=self.n)

    def step(self):
        """One reinforcement draw for all individuals."""
        total = self.B + self.R
        p_surv = self.B / total
        surv = self.rng.random(self.n) < p_surv
        self.B += surv * self.r
        self.R += (~surv) * self.r
        return surv.astype(int)

    def simulate(self):
        """
        Simulate up to max_steps with right-censoring.
        Returns:
            times: observed times
            event: True if death, False if censored
        """
        times = np.zeros(self.n, dtype=int)
        event = np.zeros(self.n, dtype=bool)
        for t in range(1, self.max_steps + 1):
            draws = self.step()
            at_risk = times == 0
            died = (draws == 0) & at_risk
            cens = (t >= self.censor_times) & at_risk
            times[died] = t; event[died] = True
            to_censor = cens & (~died)
            times[to_censor] = t; event[to_censor] = False
        still = times == 0
        times[still] = self.max_steps; event[still] = False
        return times, event

    def set_observed_data(self, times, event):
        """Load observed (times, event) for inference."""
        self.times = times
        self.event = event
        self.n = len(times)

    def log_likelihood(self, theta):
        """
        Compute log-likelihood using gamma functions.
        theta = [alpha0, D0, gamma..., delta...]
        """
        alpha0, D0 = theta[0], theta[1]
        gamma, delta = theta[2:2+self.p], theta[2+self.p:2+2*self.p]
        B0 = alpha0 * np.exp(self.X @ gamma)
        R0 = D0     * np.exp(self.X @ delta)
        ll = 0.0
        for i in range(self.n):
            t = self.times[i]
            base = gammaln((B0[i]+R0[i])/self.r) - gammaln(B0[i]/self.r)
            if self.event[i]:
                hazard = np.log(R0[i]) - np.log(B0[i]+R0[i]+self.r*(t-1))
                survival = gammaln(B0[i]/self.r + t-1) - gammaln((B0[i]+R0[i])/self.r + t-1)
                ll += base + hazard + survival
            else:
                survival = gammaln(B0[i]/self.r + t) - gammaln((B0[i]+R0[i])/self.r + t)
                ll += base + survival
        return ll

    def log_prior(self, theta):
        """Log-prior: Gamma(2,0.1) for alpha0,D0; N(0,10) for gamma,delta."""
        alpha0, D0 = theta[0], theta[1]
        gamma, delta = theta[2:2+self.p], theta[2+self.p:2+2*self.p]
        if alpha0 <= 0 or D0 <= 0:
            return -np.inf
        lp = (1*np.log(alpha0) - 0.1*alpha0) + (1*np.log(D0) - 0.1*D0)
        lp += np.sum(norm.logpdf(gamma, 0, 10))  # Increased std to 10 for better exploration
        lp += np.sum(norm.logpdf(delta, 0, 10))
        return lp

    def metropolis_update(self, n_iter=10000, burnin=2000):
        """
        Robust Metropolis–Hastings with adaptive covariance and stability fixes.
        """
        params = np.concatenate([[self.alpha0, self.D0], self.gamma, self.delta])
        ndim = len(params)
        chain = np.zeros((n_iter, ndim))
        current_ll = self.log_likelihood(params)
        current_post = current_ll + self.log_prior(params)

        # Adaptive covariance initialization with regularization
        cov = np.eye(ndim) * 0.1 + 1e-4 * np.eye(ndim)
        min_adapt = 100  # Minimum iterations before adapting

        for i in range(n_iter):
            # Propose new parameters
            try:
                prop = params + self.rng.multivariate_normal(np.zeros(ndim), cov)
            except:
                # Fallback to diagonal covariance if Cholesky fails
                prop = params + self.rng.normal(0, 0.1, ndim)

            # Skip invalid proposals
            if prop[0] <= 0 or prop[1] <= 0:
                chain[i] = params
                continue

            prop_ll = self.log_likelihood(prop)
            if np.isinf(prop_ll):  # Handle infinite likelihood
                chain[i] = params
                continue

            prop_post = prop_ll + self.log_prior(prop)

            # Metropolis acceptance
            log_ratio = prop_post - current_post
            if np.log(self.rng.random()) < log_ratio:
                params, current_ll, current_post = prop, prop_ll, prop_post

            chain[i] = params

            # Adaptive covariance update (safer implementation)
            if i > min_adapt and i % 200 == 0:
                # Use sufficient samples for covariance estimation
                sample = chain[max(0, i-1000):i]
                if len(sample) > ndim:  # Ensure enough samples
                    new_cov = np.cov(sample, rowvar=False)
                    # Regularize and update
                    cov = 0.95 * cov + 0.05 * (new_cov + 1e-4 * np.eye(ndim))

        # Store post–burnin samples
        self.param_history['alpha0'] = chain[burnin:, 0]
        self.param_history['D0']    = chain[burnin:, 1]
        self.param_history['gamma'] = chain[burnin:, 2:2+self.p]
        self.param_history['delta'] = chain[burnin:, 2+self.p:2+2*self.p]
        return chain

    def predict_hazard(self):
        """Linear predictor risk scores = X @ (delta - gamma)."""
        gamma = np.mean(self.param_history['gamma'], axis=0)
        delta = np.mean(self.param_history['delta'], axis=0)
        return self.X @ (delta - gamma)

    def concordance_index(self, risk_scores):
        """Harrell's C-index for discrimination (optimized)."""
        n = len(self.times)
        comparable = 0
        concordant = 0

        # Precompute valid pairs
        for i in range(n):
            if not self.event[i]:
                continue
            for j in range(n):
                # Only compare when j has longer survival time or same time but censored
                if (self.times[j] > self.times[i]) or (self.times[j] == self.times[i] and not self.event[j]):
                    comparable += 1
                    if risk_scores[i] > risk_scores[j]:
                        concordant += 1
                    elif risk_scores[i] == risk_scores[j]:
                        concordant += 0.5

        return concordant / comparable if comparable > 0 else 0.5

    def rmse(self, true, est):
        return np.sqrt(np.mean((true - est)**2))

    def effective_sample_size(self, samples):
        """Robust ESS calculation with autocorrelation truncation."""
        n = len(samples)
        if n <= 1:
            return 1

        mean = np.mean(samples)
        var = np.var(samples, ddof=1)
        if var == 0:
            return 1

        # Compute autocorrelations safely
        max_lag = min(1000, n-1)
        acf = np.zeros(max_lag)
        for lag in range(1, max_lag):
            if lag < n:
                cov = np.mean((samples[:n-lag] - mean) * (samples[lag:] - mean))
                acf[lag] = cov / var
                if acf[lag] < 0:  # Stop at first negative autocorrelation
                    max_lag = lag
                    break

        # Truncate and compute ESS
        tau = 1 + 2 * np.sum(acf[:max_lag])
        return n / tau

    def gelman_rubin(self, chains):
        """Robust Gelman-Rubin diagnostic implementation."""
        m = len(chains)
        n = chains[0].shape[0]
        ndim = chains[0].shape[1]

        # Calculate between-chain variance
        chain_means = np.array([np.mean(chain, axis=0) for chain in chains])
        overall_mean = np.mean(chain_means, axis=0)
        B = n / (m - 1) * np.sum((chain_means - overall_mean)**2, axis=0)

        # Calculate within-chain variance
        chain_vars = np.array([np.var(chain, axis=0, ddof=1) for chain in chains])
        W = np.mean(chain_vars, axis=0)

        # Estimated variance with regularization
        var_hat = (n - 1)/n * W + B/n + 1e-10
        R_hat = np.sqrt(var_hat / W)
        return R_hat

# Example usage
if __name__ == "__main__":
    # Simulate covariates for 1,000 couples
    n = 1000
    rng = np.random.default_rng(2025)
    age = rng.normal(65, 5, size=n)
    bmi = rng.gamma(5, 1, size=n)
    X = np.vstack([age, bmi]).T

    # True parameters
    gamma_true = np.array([0.03, -0.02])
    delta_true = np.array([0.01,  0.02])

    model = CA_RUP(alpha0=2.0,
                   D0=2.0,
                   gamma_vec=gamma_true,
                   delta_vec=delta_true,
                   covariates=X,
                   r=1.0,
                   censor_rate=0.35,
                   max_steps=20,
                   seed=2025)

    times, event = model.simulate()
    print(f"Observed deaths: {event.sum()}, censored: {len(event)-event.sum()}")

    model.set_observed_data(times, event)
    chain = model.metropolis_update(n_iter=10000, burnin=2000)

    # Validation metrics
    g_est = np.mean(model.param_history['gamma'], axis=0)
    d_est = np.mean(model.param_history['delta'], axis=0)
    print("Gamma RMSE:", model.rmse(gamma_true, g_est))
    print("Delta RMSE:", model.rmse(delta_true, d_est))

    risk_scores = model.predict_hazard()
    print("C-index:", model.concordance_index(risk_scores))

    # Convergence diagnostics (3 chains)
    chains = []
    for _ in range(3):
        m2 = CA_RUP(2.0, 2.0, gamma_true, delta_true, X,
                    1.0, 0.35, 20, seed=rng.integers(1000000))
        m2.set_observed_data(times, event)
        chains.append(m2.metropolis_update(n_iter=5000, burnin=1000))

    R_hat = model.gelman_rubin(chains)
    print("\nConvergence Diagnostics:")
    print("R_hat values:", R_hat)

    # ESS for each parameter
    print("\nEffective Sample Sizes:")
    for name in ['alpha0','D0']:
        ess = model.effective_sample_size(model.param_history[name])
        print(f"ESS {name}: {ess:.1f}")

    for j in range(model.p):
        ess_g = model.effective_sample_size(model.param_history['gamma'][:,j])
        ess_d = model.effective_sample_size(model.param_history['delta'][:,j])
        print(f"ESS gamma[{j}]: {ess_g:.1f}")
        print(f"ESS delta[{j}]: {ess_d:.1f}")